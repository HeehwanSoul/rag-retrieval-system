# Rag Retrieval System
This repository contains the code and documentation for building a Retrieval-Augmented Generation (RAG) pipeline. The goal of this project is to upload custom data into a Qdrant database and use retrieval techniques to provide context for a Large Language Model (LLM), allowing it to generate more accurate and context-aware responses.

## Project Overview
The RAG pipeline allows for efficient retrieval of relevant document segments (chunks) from a Qdrant vector database, which are then used as context to enhance the responses generated by a LLM. The project demonstrates the full pipeline of document chunking, embedding generation, database upload, and context-aware question answering.

## Features
- Document Chunking: Split PDFs or other files into smaller text chunks using two different methods.
- Input Length Check: Ensure that the chunks do not exceed the maximum input length of the embedder model.
- Custom Embeddings: Use a custom embedding model capable of processing sequences longer than 256 tokens.
- Vector Database Integration: Upload chunked data and embeddings into a Qdrant vector database for retrieval.
- Contextual Answer Generation: Use a locally running LLM to generate answers to questions, using retrieved context from the Qdrant database.
- Optimized Code: Clean, modular, and optimized Python code for better performance and readability.

## Contents
The repository contains the following:

- _From_Chunking_To_Uploading_Into_Qdrant.ipynb_: This notebook handles the complete workflow of processing documents, including chunking them into manageable segments, checking the input length against the model's token limits, generating custom embeddings, and uploading the results into a Qdrant vector database for retrieval and search integration.
- _rag.py_: The rag.py file implements a Retrieval-Augmented Generation (RAG) pipeline, combining neural search with a sentence transformer for retrieving relevant document chunks from a Qdrant database and using a causal language model to generate context-based answers to user queries. It also includes functionality to process user inputs, retrieve relevant information, and generate responses, while monitoring execution time.
- _README.md_: The file you are reading now.

The two PDF files used for chunking are:
- _angewandte_mathematik.pdf_: This document is a module handbook for the Bachelor's program in Applied and Computational Mathematics from Beuth Hochschule, covering courses such as Calculus, Linear Algebra, and Numerical Mathematics​.
- _data_science.pdf_: This document is a module handbook for the Master's program in Data Science, also from Beuth Hochschule, with topics including Machine Learning, Statistical Computing, and Big Data systems​.
